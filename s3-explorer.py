import boto3
from botocore.exceptions import ClientError
from tabulate import tabulate
from collections import defaultdict
from datetime import datetime
import csv
import sys
import concurrent.futures
import time
import argparse

# AWS CLI í”„ë¡œí•„ ê¸°ë³¸ê°’ ì„¤ì •
DEFAULT_PROFILE = None  # ê¸°ë³¸ê°’ì€ Noneìœ¼ë¡œ ì„¤ì •í•˜ì—¬ AWS CLIì˜ ê¸°ë³¸ í”„ë¡œí•„ ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©

# ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ ë³„ ìš”ìœ¨ (GB ê¸°ì¤€, ì˜ˆì‹œ ê°’)
storage_class_rates = {
    "STANDARD": 0.025,
    "STANDARD_IA": 0.0138,
    "ONEZONE_IA": 0.01,
    "GLACIER": 0.0045,
    "DEEP_ARCHIVE": 0.002,
    "INTELLIGENT_TIERING": 0.023,
    "REDUCED_REDUNDANCY": 0.023,
    "GLACIER_IR": 0.005
}
# ëŒ€ëŸ‰ ì‚¬ìš© í• ì¸ ì ìš©ì„ ìœ„í•œ ë‹¨ê³„ë³„ ìš”ìœ¨
def calculate_discounted_cost(total_gb):
    if total_gb <= 50 * 1024:
        return total_gb * 0.023  # 0 ~ 50 TB
    elif total_gb <= 450 * 1024:
        return total_gb * 0.0225  # 50 ~ 450 TB
    elif total_gb <= 500 * 1024:
        return total_gb * 0.0215  # 450 ~ 500 TB
    else:
        return total_gb * 0.02  # 500 TB ì´ìƒ

def analyze_bucket(bucket_name, creation_date, session, fast_mode=False):
    """ë‹¨ì¼ ë²„í‚· ë¶„ì„ í•¨ìˆ˜"""
    s3 = session.client('s3')
    start_time = time.time()
    print(f"ğŸ” ë¶„ì„ ì‹œì‘: {bucket_name}")
    
    storage_class_size = defaultdict(int)
    
    # ë¦¬ì „ ê°€ì ¸ì˜¤ê¸°
    try:
        region = s3.get_bucket_location(Bucket=bucket_name)['LocationConstraint'] or 'us-east-1'
    except Exception as e:
        region = 'Unknown'
        print(f"âš ï¸ ë¦¬ì „ í™•ì¸ ì˜¤ë¥˜ {bucket_name}: {e}")
        
    # íƒœê·¸ ê²€ìƒ‰ - S3 ë²„í‚· íƒœê·¸ê°€ ìˆëŠ” ê²½ìš° ê°€ì ¸ì˜¤ê¸°
    try:
        tags = s3.get_bucket_tagging(Bucket=bucket_name)
        tags_dict = {tag['Key']: tag['Value'] for tag in tags.get('TagSet', [])}
    except Exception:
        tags_dict = {}
    
    # S3 ì¸ë²¤í† ë¦¬ ì„¤ì •ì´ ìˆëŠ”ì§€ í™•ì¸ (ë‚˜ì¤‘ì— í™œìš© ê°€ëŠ¥)
    has_inventory = False
    try:
        inventory_configs = s3.list_bucket_inventory_configurations(Bucket=bucket_name)
        if 'InventoryConfigurationList' in inventory_configs and inventory_configs['InventoryConfigurationList']:
            has_inventory = True
    except Exception:
        pass
    
    # CloudWatch ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ì˜ í¬ê¸° ì¶”ì • ì‹œë„ (ë” ë¹ ë¥¸ ë°©ë²•)
    try:
        cloudwatch = session.client('cloudwatch')
        now = datetime.utcnow()
        
        # ëª¨ë“  ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ ìœ í˜• ì •ì˜
        storage_types = [
            'StandardStorage',           # í‘œì¤€
            'StandardIAStorage',         # í‘œì¤€-IA
            'StandardOneZoneIAStorage',  # ë‹¨ì¼ ì˜ì—­-IA
            'ReducedRedundancyStorage',  # RRS (ë ˆê±°ì‹œ)
            'GlacierStorage',            # Glacier
            'GlacierIRStorage',          # Glacier IR
            'GlacierDeepArchiveStorage', # Glacier Deep Archive
            'IntelligentTieringFAStorage', # Intelligent-Tiering
            'IntelligentTieringIAStorage', # Intelligent-Tiering IA
            'IntelligentTieringAAStorage', # Intelligent-Tiering Archive Access
            'IntelligentTieringDAAStorage', # Intelligent-Tiering Deep Archive Access
        ]
        
        # ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ ë§¤í•‘ (CloudWatch íƒ€ì… -> S3 API íƒ€ì…)
        storage_class_mapping = {
            'StandardStorage': 'STANDARD',
            'StandardIAStorage': 'STANDARD_IA',
            'StandardOneZoneIAStorage': 'ONEZONE_IA',
            'ReducedRedundancyStorage': 'REDUCED_REDUNDANCY',
            'GlacierStorage': 'GLACIER',
            'GlacierIRStorage': 'GLACIER_IR',
            'GlacierDeepArchiveStorage': 'DEEP_ARCHIVE',
            'IntelligentTieringFAStorage': 'INTELLIGENT_TIERING',
            'IntelligentTieringIAStorage': 'INTELLIGENT_TIERING',
            'IntelligentTieringAAStorage': 'INTELLIGENT_TIERING',
            'IntelligentTieringDAAStorage': 'INTELLIGENT_TIERING'
        }
        
        has_metrics = False
        
        # ê° ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ë³„ë¡œ ë©”íŠ¸ë¦­ ì¡°íšŒ
        for storage_type in storage_types:
            response = cloudwatch.get_metric_statistics(
                Namespace='AWS/S3',
                MetricName='BucketSizeBytes',
                Dimensions=[
                    {'Name': 'BucketName', 'Value': bucket_name},
                    {'Name': 'StorageType', 'Value': storage_type}
                ],
                StartTime=now.replace(day=1, hour=0, minute=0, second=0, microsecond=0),
                EndTime=now,
                Period=86400,
                Statistics=['Average'],
            )
            
            if response['Datapoints']:
                has_metrics = True
                class_size = max([point['Average'] for point in response['Datapoints']])
                
                # í•´ë‹¹ ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ì— í¬ê¸° í• ë‹¹
                s3_storage_class = storage_class_mapping.get(storage_type, 'STANDARD')
                storage_class_size[s3_storage_class] += class_size
                print(f"  - {storage_type}: {class_size / (1024**3):.2f} GB")
        
        if has_metrics:
            print(f"âœ… {bucket_name}: ëª¨ë“  ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ ë©”íŠ¸ë¦­ ì‚¬ìš© ì™„ë£Œ ({time.time() - start_time:.2f}ì´ˆ)")
        elif not fast_mode:
            # ë©”íŠ¸ë¦­ì´ ì—†ìœ¼ë©´ ê°ì²´ ë‚˜ì—´ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´
            paginator = s3.get_paginator('list_objects_v2')
            
            # ìµœì í™”: ì²« í˜ì´ì§€ë§Œ ê°€ì ¸ì™€ì„œ ê°ì²´ ì´ ê°œìˆ˜ í™•ì¸
            first_page = next(paginator.paginate(Bucket=bucket_name, MaxItems=1000), None)
            if first_page and first_page.get('KeyCount', 0) > 10000:
                # ê°ì²´ê°€ ë§ìœ¼ë©´ HEAD ìš”ì²­ìœ¼ë¡œ ë²„í‚· í¬ê¸°ë§Œ í™•ì¸ (ë¹ ë¥¸ ì¶”ì •)
                print(f"âš¡ {bucket_name}ì— ê°ì²´ê°€ ë§ìŒ: ë¹ ë¥¸ ì¶”ì • ëª¨ë“œë¡œ ì „í™˜")
                response = s3.head_bucket(Bucket=bucket_name)
                
                # ìƒ˜í”Œë§ìœ¼ë¡œ ì¶”ì • - ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ë³„ ë¶„í¬ íŒŒì•…
                storage_classes_count = defaultdict(int)
                total_sampled = 0
                sampled_size = 0
                
                # ìƒ˜í”Œ ë°ì´í„° ìˆ˜ì§‘ (ì²˜ìŒ 1000ê°œ ê°ì²´)
                for page in paginator.paginate(Bucket=bucket_name, PaginationConfig={'MaxItems': 1000}):
                    for obj in page.get('Contents', []):
                        size = obj['Size']
                        storage_class = obj.get('StorageClass', 'STANDARD')
                        storage_class_size[storage_class] += size
                        storage_classes_count[storage_class] += 1
                        sampled_size += size
                        total_sampled += 1
                
                # ì „ì²´ ê°ì²´ ìˆ˜ ê°€ì ¸ì˜¤ê¸° (HEAD ìš”ì²­ìœ¼ë¡œ ë¹ ë¥´ê²Œ í™•ì¸)
                total_estimated_count = first_page.get('KeyCount', 0)
                
                # ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ë³„ ë¶„í¬ ê¸°ë°˜ ì „ì²´ í¬ê¸° ì¶”ì •
                if total_sampled > 0:
                    # ê° ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ë³„ ë¹„ìœ¨ ê³„ì‚° ë° ì´ í¬ê¸° ì¶”ì •
                    for storage_class, count in storage_classes_count.items():
                        ratio = count / total_sampled
                        estimated_size = (sampled_size / total_sampled) * total_estimated_count * ratio
                        storage_class_size[storage_class] = estimated_size
                    print(f"  - ìƒ˜í”Œë§: {total_sampled}ê°œ ê°ì²´ ë¶„ì„ìœ¼ë¡œ {total_estimated_count}ê°œ ì¶”ì •")
                
                print(f"âœ… {bucket_name}: ìƒ˜í”Œë§ ê¸°ë°˜ ì¶”ì • ì™„ë£Œ ({time.time() - start_time:.2f}ì´ˆ)")
            elif first_page:
                # ê°ì²´ê°€ ì ìœ¼ë©´ ì •ìƒì ìœ¼ë¡œ ì „ì²´ ë‚˜ì—´
                for page in paginator.paginate(Bucket=bucket_name):
                    for obj in page.get('Contents', []):
                        size = obj['Size']
                        storage_class = obj.get('StorageClass', 'STANDARD')
                        storage_class_size[storage_class] += size

                print(f"âœ… {bucket_name}: ì „ì²´ ê°ì²´ ë¶„ì„ ì™„ë£Œ ({time.time() - start_time:.2f}ì´ˆ)")
            else:
                # ì ‘ê·¼ ê¶Œí•œì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ë²„í‚·
                print(f"âš ï¸ {bucket_name}: ë¹„ì–´ìˆê±°ë‚˜ ì ‘ê·¼ ê¶Œí•œì´ ì—†ëŠ” ë²„í‚·")
    except Exception as e:
        print(f"âš ï¸ {bucket_name} ë¶„ì„ ì˜¤ë¥˜: {e}")

    # ìˆ˜ëª…ì£¼ê¸° ì •ì±… í™•ì¸
    try:
        lifecycle = s3.get_bucket_lifecycle_configuration(Bucket=bucket_name)
        rules = lifecycle.get('Rules', [])
        if rules:
            rule_summary = f"{len(rules)} rules"
            # ë‹¨ìˆ˜í˜•ê³¼ ë³µìˆ˜í˜• ëª¨ë‘ í™•ì¸
            has_transition = any('Transition' in rule or 'Transitions' in rule for rule in rules)
            has_expiration = any('Expiration' in rule or 'Expirations' in rule for rule in rules)
            rule_summary += f", T:{has_transition}, E:{has_expiration}"
            lifecycle_policy = rule_summary
        else:
            lifecycle_policy = "NO"
    except ClientError as e:
        if e.response['Error']['Code'] == 'NoSuchLifecycleConfiguration':
            lifecycle_policy = "NO"
        else:
            lifecycle_policy = "Error"

    # ì´ ìš©ëŸ‰ ë° ë¹„ìš© ê³„ì‚°
    total_size = sum(storage_class_size.values())
    total_gb = total_size / (1024 ** 3)

    # ëŒ€ëŸ‰ ì‚¬ìš© í• ì¸ ì ìš©
    estimated_cost = calculate_discounted_cost(total_gb)
    
    # ê²°ê³¼ ë°˜í™˜
    return {
        'bucket_name': bucket_name,
        'creation_date': creation_date,
        'region': region,
        'total_gb': total_gb,
        'lifecycle_policy': lifecycle_policy,
        'estimated_cost': estimated_cost,
        'storage_class_size': dict(storage_class_size),
        'tags': tags_dict
    }

def main():
    start_time = time.time()
    print("ğŸ“¦ S3 Bucket Size Check ì‹œì‘...\n")
    
    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description='S3 ë²„í‚· ìš©ëŸ‰ ë¶„ì„ ë° ë¹„ìš© ì¶”ì • ë„êµ¬')
    parser.add_argument('-w', '--workers', type=int, default=10, 
                      help='ë™ì‹œ ì²˜ë¦¬í•  ìµœëŒ€ ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸ê°’: 10)')
    parser.add_argument('-f', '--fast', action='store_true',
                      help='ê³ ì† ëª¨ë“œ: CloudWatch ë©”íŠ¸ë¦­ë§Œ ì‚¬ìš©í•˜ê³  ê°ì²´ ë‚˜ì—´ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('-r', '--region', type=str,
                      help='íŠ¹ì • ë¦¬ì „ì˜ ë²„í‚·ë§Œ ë¶„ì„')
    parser.add_argument('-n', '--name', type=str,
                      help='ì´ë¦„ì— íŠ¹ì • ë¬¸ìì—´ì„ í¬í•¨í•˜ëŠ” ë²„í‚·ë§Œ ë¶„ì„')
    parser.add_argument('-t', '--top', type=int,
                      help='í¬ê¸°ê°€ ê°€ì¥ í° Nê°œì˜ ë²„í‚·ë§Œ ë¶„ì„ (ê²°ê³¼ ì •ë ¬ í›„)')
    parser.add_argument('-p', '--profile', type=str, default=DEFAULT_PROFILE,
                      help='ì‚¬ìš©í•  AWS í”„ë¡œí•„ (ê¸°ë³¸ê°’: ê¸°ë³¸ AWS í”„ë¡œí•„)')
    
    args = parser.parse_args()
    
    # í”„ë¡œí•„ ì„¤ì •
    aws_profile_to_use = args.profile
    
    # boto3 ì„¸ì…˜ ìƒì„±
    session = boto3.Session(profile_name=aws_profile_to_use)
    s3 = session.client('s3')
    
    # ëª¨ë“  ë²„í‚· ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    buckets = s3.list_buckets()['Buckets']
    
    # í•„í„°ë§ ì ìš©
    if args.name:
        print(f"í•„í„°: '{args.name}'ì„ í¬í•¨í•˜ëŠ” ë²„í‚·ë§Œ ë¶„ì„")
        filtered_buckets = [b for b in buckets if args.name in b['Name']]
    else:
        filtered_buckets = buckets
    
    if args.region:
        print(f"í•„í„°: '{args.region}' ë¦¬ì „ì˜ ë²„í‚·ë§Œ ë¶„ì„")
        region_filtered = []
        for bucket in filtered_buckets:
            try:
                region = s3.get_bucket_location(Bucket=bucket['Name'])['LocationConstraint'] or 'us-east-1'
                if region == args.region:
                    region_filtered.append(bucket)
            except Exception:
                pass
        filtered_buckets = region_filtered
    
    bucket_count = len(filtered_buckets)
    print(f"ì´ {bucket_count}ê°œì˜ ë²„í‚· ë¶„ì„ ëŒ€ìƒ")
    
    # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‘ì—… ëª©ë¡ ì¤€ë¹„
    bucket_data = []
    
    # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
    max_workers = min(args.workers, bucket_count)
    print(f"ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (ìµœëŒ€ {max_workers}ê°œ ë™ì‹œ ì²˜ë¦¬)")
    
    # ê³ ì† ëª¨ë“œ ì•Œë¦¼
    if args.fast:
        print("âš¡ ê³ ì† ëª¨ë“œ í™œì„±í™”: CloudWatch ë©”íŠ¸ë¦­ë§Œ ì‚¬ìš©í•˜ì—¬ ë¹ ë¥´ê²Œ ë¶„ì„í•©ë‹ˆë‹¤")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ê° ë²„í‚·ì— ëŒ€í•œ ì‘ì—… ì œì¶œ
        future_to_bucket = {
            executor.submit(analyze_bucket, bucket['Name'], bucket['CreationDate'].strftime("%Y-%m-%d"), session, args.fast): bucket['Name']
            for bucket in filtered_buckets
        }
        
        # ì‘ì—… ì™„ë£Œ ëŒ€ê¸° ë° ê²°ê³¼ ìˆ˜ì§‘
        completed = 0
        for future in concurrent.futures.as_completed(future_to_bucket):
            bucket_name = future_to_bucket[future]
            try:
                result = future.result()
                
                # ë²„í‚·ì˜ ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ë³„ í¬ê¸° ì •ë³´
                storage_class_sizes = {}
                for storage_class, size in result['storage_class_size'].items():
                    size_gb = size / (1024 ** 3)
                    if size_gb >= 0.01:  # 0.01GB(10MB) ì´ìƒì¸ ê²½ìš°ë§Œ í‘œì‹œ
                        storage_class_sizes[storage_class] = f"{size_gb:.2f} GB"
                
                # ë²„í‚· ë°ì´í„°ì— ì¶”ê°€
                bucket_data.append([
                    result['bucket_name'],
                    result['creation_date'],
                    result['region'],
                    f"{result['total_gb']:,.2f} GB",
                    storage_class_sizes,  # ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ë³„ í¬ê¸° ì •ë³´
                    result['lifecycle_policy'],
                    f"${result['estimated_cost']:,.2f}"
                ])
                completed += 1
                print(f"ì§„í–‰ ìƒí™©: {completed}/{bucket_count} ({completed/bucket_count*100:.1f}%)")
            except Exception as e:
                print(f"âš ï¸ {bucket_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ë²„í‚· í¬ê¸° ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    bucket_data.sort(key=lambda x: float(x[3].split()[0].replace(',', '')), reverse=True)
    
    # ë§Œì•½ top ì˜µì…˜ì´ ìˆë‹¤ë©´ ê²°ê³¼ë¥¼ í¬ê¸° ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ Nê°œë§Œ ì„ íƒ
    if args.top and args.top < len(bucket_data):
        # í¬ê¸°ë¡œ ì •ë ¬ (ì´ë¯¸ ì •ë ¬ë˜ì–´ ìˆì§€ë§Œ ì¬í™•ì¸)
        bucket_data.sort(key=lambda x: float(x[3].split()[0].replace(',', '')), reverse=True)
        print(f"\nìƒìœ„ {args.top}ê°œ í° ë²„í‚·ë§Œ í‘œì‹œí•©ë‹ˆë‹¤ (ì „ì²´ {len(bucket_data)}ê°œ ì¤‘)")
        bucket_data = bucket_data[:args.top]
    
    # ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ë³„ ìš”ì•½ ì •ë³´ ì¶”ê°€
    storage_class_totals = defaultdict(float)
    for future in future_to_bucket:
        try:
            result = future.result()
            for storage_class, size in result['storage_class_size'].items():
                storage_class_totals[storage_class] += size / (1024 ** 3)  # GBë¡œ ë³€í™˜
        except Exception:
            pass
    
    # ì¶œë ¥
    headers = [
        "Bucket Name", "Created", "Region", "Total Size (GB)", "Storage Classes", "Lifecycle Policy",
        "Estimated Cost (USD)"
    ]
    
    # ì¶œë ¥ í˜•ì‹ ì¡°ì • - ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    for row in bucket_data:
        storage_classes_info = row[4]  # ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ ì •ë³´ (ë”•ì…”ë„ˆë¦¬)
        
        # ë”•ì…”ë„ˆë¦¬ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        if storage_classes_info:
            storage_classes_text = "\n".join([f"{cls}: {size}" for cls, size in storage_classes_info.items()])
        else:
            storage_classes_text = "N/A"
        
        # ë”•ì…”ë„ˆë¦¬ë¥¼ ë¬¸ìì—´ë¡œ ëŒ€ì²´
        row[4] = storage_classes_text
    
    print("\nğŸ“Š === S3 Bucket Summary (Monthly) ===\n")
    print(tabulate(bucket_data, headers=headers, tablefmt="fancy_grid", colalign=["center", "center", "center", "right", "left", "center", "right"]))
    
    # ì´í•© ê³„ì‚° (ì¸ë±ìŠ¤ ìˆ˜ì •: ì´ í¬ê¸°ëŠ” 3ë²ˆ ì¸ë±ìŠ¤, ë¹„ìš©ì€ 6ë²ˆ ì¸ë±ìŠ¤)
    total_size_gb = sum([float(row[3].split()[0].replace(',', '')) for row in bucket_data])
    total_cost = sum([float(row[6].replace('$', '').replace(',', '')) for row in bucket_data])
    print("-" * 80)
    print(f"{'TOTAL':<40} {total_size_gb:>15,.2f} GB {total_cost:>20,.2f} USD")
    
    # ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ë³„ ìš”ì•½ ì¶œë ¥
    print("\nğŸ“Š === ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ë³„ ìš”ì•½ ===")
    storage_class_data = []
    for storage_class, total_gb in sorted(storage_class_totals.items(), key=lambda x: x[1], reverse=True):
        # ê° ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ë³„ ë¹„ìš© ê³„ì‚°
        rate = storage_class_rates.get(storage_class, storage_class_rates["STANDARD"])
        class_cost = total_gb * rate
        storage_class_data.append([
            storage_class,
            f"{total_gb:,.2f} GB",
            f"{(total_gb/total_size_gb*100) if total_size_gb else 0:.1f}%",
            f"${class_cost:,.2f}",
            f"${rate:.4f}/GB"
        ])
    
    print(tabulate(storage_class_data, 
                headers=["Storage Class", "Size (GB)", "% of Total", "Est. Cost (USD)", "Rate"],
                tablefmt="fancy_grid"))
    
    # CSV ì €ì¥ìš© ë°ì´í„° ì¤€ë¹„ - ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ ì •ë³´ë¥¼ CSV í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    csv_data = []
    for row in bucket_data:
        # ì›ë³¸ ë°ì´í„° ë³µì‚¬
        csv_row = row.copy()
        
        # ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ ì •ë³´ í¬ë§·íŒ… (CSVì—ì„œëŠ” ì¤„ë°”ê¿ˆì´ ë¬¸ì œë  ìˆ˜ ìˆìŒ)
        storage_classes = csv_row[4]
        if isinstance(storage_classes, str):  # ì´ë¯¸ ë¬¸ìì—´ë¡œ ë³€í™˜ëœ ê²½ìš°
            csv_row[4] = storage_classes.replace("\n", " | ")
        
        csv_data.append(csv_row)
    
    # CSV ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f's3_report_{timestamp}.csv'
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(headers)
        writer.writerows(csv_data)
        
        # ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ ìš”ì•½ ì •ë³´ë„ ì¶”ê°€
        writer.writerow([])
        writer.writerow(["Storage Class Summary"])
        writer.writerow(["Storage Class", "Size (GB)", "% of Total", "Est. Cost (USD)", "Rate"])
        writer.writerows(storage_class_data)
    
    print(f"\nâœ… ë¦¬í¬íŠ¸ê°€ {filename} ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"â±ï¸ ì´ ì‹¤í–‰ ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")

if __name__ == "__main__":
    main()